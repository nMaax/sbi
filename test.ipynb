{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "import torch\n",
    "from scipy.stats import gaussian_kde\n",
    "from torch import eye, ones, zeros\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.analysis import conditional_potential\n",
    "from sbi.inference import (\n",
    "    FMPE,\n",
    "    NPSE,\n",
    "    MCMCPosterior,\n",
    "    VectorFieldPosterior,\n",
    "    simulate_for_sbi,\n",
    "    vector_field_estimator_based_potential,\n",
    ")\n",
    "from sbi.simulators import linear_gaussian\n",
    "from sbi.simulators.linear_gaussian import (\n",
    "    samples_true_posterior_linear_gaussian_mvn_prior_different_dims,\n",
    "    samples_true_posterior_linear_gaussian_uniform_prior,\n",
    "    true_posterior_linear_gaussian_mvn_prior,\n",
    ")\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils.metrics import check_c2st\n",
    "from sbi.utils.user_input_checks import process_simulator\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ------------------------------- FAST TESTS -----------------------------------\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# We always test num_dim and sample_with with defaults and mark the rests as slow.\n",
    "@pytest.mark.parametrize(\n",
    "    \"vector_field_type, num_dim, prior_str, sample_with\",\n",
    "    [\n",
    "        (\"vp\", 1, \"gaussian\", [\"sde\", \"ode\"]),\n",
    "        (\"vp\", 3, \"uniform\", [\"sde\", \"ode\"]),\n",
    "        (\"vp\", 3, \"gaussian\", [\"sde\", \"ode\"]),\n",
    "        (\"ve\", 3, \"uniform\", [\"sde\", \"ode\"]),\n",
    "        (\"subvp\", 3, \"uniform\", [\"sde\", \"ode\"]),\n",
    "        (\"fmpe\", 1, \"gaussian\", [\"sde\", \"ode\"]),\n",
    "        (\"fmpe\", 1, \"uniform\", [\"sde\", \"ode\"]),\n",
    "        (\"fmpe\", 3, \"gaussian\", [\"sde\", \"ode\"]),\n",
    "        (\"fmpe\", 3, \"uniform\", [\"sde\", \"ode\"]),\n",
    "    ],\n",
    ")\n",
    "def test_c2st_vector_field_on_linearGaussian(\n",
    "    vector_field_type, num_dim: int, prior_str: str, sample_with: List[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Test whether NPSE and FMPE infer well a simple example with available ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    x_o = zeros(1, num_dim)\n",
    "    num_samples = 1000\n",
    "    num_simulations = 10_000\n",
    "\n",
    "    # likelihood_mean will be likelihood_shift+theta\n",
    "    likelihood_shift = -1.0 * ones(num_dim)\n",
    "    likelihood_cov = 0.3 * eye(num_dim)\n",
    "\n",
    "    if prior_str == \"gaussian\":\n",
    "        prior_mean = zeros(num_dim)\n",
    "        prior_cov = eye(num_dim)\n",
    "        prior = MultivariateNormal(loc=prior_mean, covariance_matrix=prior_cov)\n",
    "        gt_posterior = true_posterior_linear_gaussian_mvn_prior(\n",
    "            x_o, likelihood_shift, likelihood_cov, prior_mean, prior_cov\n",
    "        )\n",
    "        target_samples = gt_posterior.sample((num_samples,))\n",
    "    else:\n",
    "        prior = utils.BoxUniform(-2.0 * ones(num_dim), 2.0 * ones(num_dim))\n",
    "        target_samples = samples_true_posterior_linear_gaussian_uniform_prior(\n",
    "            x_o,\n",
    "            likelihood_shift,\n",
    "            likelihood_cov,\n",
    "            prior=prior,\n",
    "            num_samples=num_samples,\n",
    "        )\n",
    "    if vector_field_type == \"fmpe\":\n",
    "        inference = FMPE(prior, show_progress_bars=True)\n",
    "    else:\n",
    "        inference = NPSE(prior, sde_type=vector_field_type, show_progress_bars=True)\n",
    "\n",
    "    theta = prior.sample((num_simulations,))\n",
    "    x = linear_gaussian(theta, likelihood_shift, likelihood_cov)\n",
    "\n",
    "    score_estimator = inference.append_simulations(theta, x).train(\n",
    "        training_batch_size=100,\n",
    "        max_num_epochs=50,\n",
    "    )\n",
    "    # amortize the training when testing sample_with.\n",
    "    for method in sample_with:\n",
    "        posterior = inference.build_posterior(\n",
    "            score_estimator,\n",
    "            sample_with=method,\n",
    "            neural_ode_backend=\"zuko\",\n",
    "        )\n",
    "        posterior.set_default_x(x_o)\n",
    "        samples = posterior.sample((num_samples,))\n",
    "\n",
    "        # Compute the c2st and assert it is near chance level of 0.5.\n",
    "        check_c2st(\n",
    "            samples,\n",
    "            target_samples,\n",
    "            alg=f\"vector_field-{vector_field_type}-{prior_str}-{num_dim}D-{method}\",\n",
    "            tol=0.15 if method == \"ode\" else 0.1,  # ODE with scores is less accurate\n",
    "        )\n",
    "\n",
    "    # Checks for log_prob()\n",
    "    if prior_str == \"gaussian\":\n",
    "        # For the Gaussian prior, we compute the KLd between ground truth and\n",
    "        # posterior.\n",
    "\n",
    "        # Disable exact integration for the ODE solver to speed up the computation.\n",
    "        posterior.potential_fn.neural_ode.update_params(\n",
    "            exact=False,\n",
    "            atol=1e-4,\n",
    "            rtol=1e-4,\n",
    "        )\n",
    "        dkl = get_dkl_gaussian_prior(\n",
    "            posterior,\n",
    "            x_o[0],\n",
    "            likelihood_shift,\n",
    "            likelihood_cov,\n",
    "            prior_mean,\n",
    "            prior_cov,\n",
    "        )\n",
    "\n",
    "        max_dkl = 0.15\n",
    "\n",
    "        assert dkl < max_dkl, (\n",
    "            f\"D-KL={dkl} is more than 2 stds above the average performance.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 33"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m theta \u001b[38;5;241m=\u001b[39m prior\u001b[38;5;241m.\u001b[39msample((num_simulations,))\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m linear_gaussian(theta, likelihood_shift, likelihood_cov)\n\u001b[0;32m---> 40\u001b[0m score_estimator \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_simulations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbi_versions/sbi/sbi/inference/trainers/vfpe/base_vf_inference.py:441\u001b[0m, in \u001b[0;36mVectorFieldInference.train\u001b[0;34m(self, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, ema_loss_decay, validation_times, resume_training, force_first_round_loss, discard_prior_samples, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m         validation_times_rep \u001b[38;5;241m=\u001b[39m validation_times\u001b[38;5;241m.\u001b[39mrepeat_interleave(\n\u001b[1;32m    437\u001b[0m             val_batch_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    438\u001b[0m         )\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;66;03m# Take negative loss here to get validation log_prob.\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m         val_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproposal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcalibration_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_times_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_first_round_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_first_round_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m         val_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val_losses\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Take mean over all validation samples.\u001b[39;00m\n",
      "File \u001b[0;32m~/sbi_versions/sbi/sbi/inference/trainers/vfpe/base_vf_inference.py:667\u001b[0m, in \u001b[0;36mVectorFieldInference._loss\u001b[0;34m(self, theta, x, masks, proposal, calibration_kernel, times, force_first_round_loss)\u001b[0m\n\u001b[1;32m    664\u001b[0m cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_round \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m force_first_round_loss:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# First round loss.\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_neural_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    670\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-round \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with arbitrary proposals is not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m     )\n",
      "File \u001b[0;32m~/sbi_versions/sbi/sbi/neural_nets/estimators/score_estimator.py:259\u001b[0m, in \u001b[0;36mConditionalScoreEstimator.loss\u001b[0;34m(self, input, condition, times, control_variate, control_variate_threshold)\u001b[0m\n\u001b[1;32m    256\u001b[0m score_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39meps \u001b[38;5;241m/\u001b[39m std\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Predict score from noised input and diffusion time.\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m score_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_noised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Compute weights over time.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_fn(times)\n",
      "File \u001b[0;32m~/sbi_versions/sbi/sbi/neural_nets/estimators/score_estimator.py:183\u001b[0m, in \u001b[0;36mConditionalScoreEstimator.forward\u001b[0;34m(self, input, condition, time)\u001b[0m\n\u001b[1;32m    179\u001b[0m condition_emb \u001b[38;5;241m=\u001b[39m condition_emb\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mcondition_emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;28mlen\u001b[39m(batch_shape) :]\n\u001b[1;32m    181\u001b[0m )\n\u001b[1;32m    182\u001b[0m time_enc \u001b[38;5;241m=\u001b[39m time_enc\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m score_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m score_pred \u001b[38;5;241m=\u001b[39m score_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mscore_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Output pre-conditioned score\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# The learnable part will be largly scaled at the beginning of the diffusion\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# and the gaussian part (where it should end up) will dominate at the end of\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# the diffusion.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sbi_versions/sbi/sbi/neural_nets/net_builders/vector_field_nets.py:596\u001b[0m, in \u001b[0;36mVectorFieldMLP.forward\u001b[0;34m(self, theta, x_emb_cond, t)\u001b[0m\n\u001b[1;32m    594\u001b[0m h_old \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m    595\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i](h)\n\u001b[0;32m--> 596\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t_emb\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_connections:\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/activation.py:734\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vector_field_type = \"ve\"\n",
    "num_dim = 3\n",
    "prior_str = \"uniform\"\n",
    "sample_with = \"sde\"\n",
    "\n",
    "\n",
    "x_o = zeros(1, num_dim)\n",
    "num_samples = 1000\n",
    "num_simulations = 100_000\n",
    "\n",
    "# likelihood_mean will be likelihood_shift+theta\n",
    "likelihood_shift = -1.0 * ones(num_dim)\n",
    "likelihood_cov = 0.3 * eye(num_dim)\n",
    "\n",
    "if prior_str == \"gaussian\":\n",
    "    prior_mean = zeros(num_dim)\n",
    "    prior_cov = eye(num_dim)\n",
    "    prior = MultivariateNormal(loc=prior_mean, covariance_matrix=prior_cov)\n",
    "    gt_posterior = true_posterior_linear_gaussian_mvn_prior(\n",
    "        x_o, likelihood_shift, likelihood_cov, prior_mean, prior_cov\n",
    "    )\n",
    "    target_samples = gt_posterior.sample((num_samples,))\n",
    "else:\n",
    "    prior = utils.BoxUniform(-2.0 * ones(num_dim), 2.0 * ones(num_dim))\n",
    "    target_samples = samples_true_posterior_linear_gaussian_uniform_prior(\n",
    "        x_o,\n",
    "        likelihood_shift,\n",
    "        likelihood_cov,\n",
    "        prior=prior,\n",
    "        num_samples=num_samples,\n",
    "    )\n",
    "if vector_field_type == \"fmpe\":\n",
    "    inference = FMPE(prior, show_progress_bars=True)\n",
    "else:\n",
    "    inference = NPSE(prior, sde_type=vector_field_type, show_progress_bars=True)\n",
    "\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = linear_gaussian(theta, likelihood_shift, likelihood_cov)\n",
    "\n",
    "score_estimator = inference.append_simulations(theta, x).train(training_batch_size=5000, max_num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff780587430>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(inference.summary[\"validation_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1438a3a2000f4569b94356cccaa94327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1000 posterior samples for 1 observations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5495, dtype=torch.float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = inference.build_posterior(\n",
    "    score_estimator,\n",
    "    sample_with=\"ode\",\n",
    "    neural_ode_backend=\"zuko\",\n",
    ")\n",
    "posterior.set_default_x(x_o)\n",
    "samples = posterior.sample((num_samples,))\n",
    "\n",
    "from sbi.utils.metrics import c2st\n",
    "\n",
    "c2st(samples, target_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.neural_nets.factory import posterior_score_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_estimator = posterior_score_nn(\n",
    "    net=\"transformer\",\n",
    "    sde_type=\"vp\",\n",
    ")(theta, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2000x5 and 128x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscore_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sbi_versions/sbi/sbi/neural_nets/net_builders/vector_field_nets.py:890\u001b[0m, in \u001b[0;36mVectorFieldTransformer.forward\u001b[0;34m(self, theta, x_emb_cond, t)\u001b[0m\n\u001b[1;32m    888\u001b[0m         h \u001b[38;5;241m=\u001b[39m block(h, x_emb_cond, cond_emb)\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# project to output dimension\u001b[39;00m\n\u001b[1;32m    893\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(h)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sbi_versions/sbi/sbi/neural_nets/net_builders/vector_field_nets.py:621\u001b[0m, in \u001b[0;36mDiTBlock.forward\u001b[0;34m(self, x, cond)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the block.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03margs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    output tensor (b, d)\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# get adaptive ln parameters\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m ada_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mada_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m attn_shift, attn_scale, attn_gate, mlp_shift, mlp_scale, mlp_gate \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    623\u001b[0m     ada_params\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m6\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    624\u001b[0m )\n\u001b[1;32m    626\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2000x5 and 128x128)"
     ]
    }
   ],
   "source": [
    "score_estimator.net(theta, x, torch.ones((x.shape[0],)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.113822937011719\n",
      "13.425664901733398\n",
      "12.987004280090332\n",
      "11.121783256530762\n",
      "11.257052421569824\n",
      "13.762019157409668\n",
      "11.79991340637207\n",
      "11.951704978942871\n",
      "11.653289794921875\n",
      "13.153412818908691\n",
      "13.103706359863281\n",
      "15.347309112548828\n",
      "12.43622875213623\n",
      "12.382999420166016\n",
      "12.676677703857422\n",
      "12.984923362731934\n",
      "12.675025939941406\n",
      "11.953932762145996\n",
      "14.743983268737793\n",
      "12.386783599853516\n",
      "10.53994369506836\n",
      "10.893366813659668\n",
      "11.351229667663574\n",
      "13.405434608459473\n",
      "13.58590030670166\n",
      "10.409858703613281\n",
      "13.266735076904297\n",
      "11.995134353637695\n",
      "14.283706665039062\n",
      "12.710716247558594\n",
      "14.932929039001465\n",
      "12.813092231750488\n",
      "12.576722145080566\n",
      "10.68285083770752\n",
      "13.628936767578125\n",
      "13.177044868469238\n",
      "13.340861320495605\n",
      "12.377195358276367\n",
      "13.569666862487793\n",
      "14.2720947265625\n",
      "12.809351921081543\n",
      "13.666046142578125\n",
      "12.843971252441406\n",
      "12.078242301940918\n",
      "14.495315551757812\n",
      "13.19033432006836\n",
      "14.182332038879395\n",
      "14.51050853729248\n",
      "11.518965721130371\n",
      "13.668880462646484\n",
      "13.713608741760254\n",
      "13.107612609863281\n",
      "13.852465629577637\n",
      "13.353832244873047\n",
      "16.550519943237305\n",
      "12.32408618927002\n",
      "14.966650009155273\n",
      "10.986567497253418\n",
      "13.66690731048584\n",
      "13.057820320129395\n",
      "12.730609893798828\n",
      "11.571743965148926\n",
      "12.851874351501465\n",
      "13.825409889221191\n",
      "13.727132797241211\n",
      "10.740845680236816\n",
      "13.304533004760742\n",
      "12.682371139526367\n",
      "15.392217636108398\n",
      "14.336634635925293\n",
      "12.799698829650879\n",
      "12.070463180541992\n",
      "12.879545211791992\n",
      "10.701648712158203\n",
      "14.147562026977539\n",
      "13.604742050170898\n",
      "12.45219612121582\n",
      "10.893806457519531\n",
      "11.00584602355957\n",
      "10.999558448791504\n",
      "11.688488006591797\n",
      "12.628063201904297\n",
      "13.692757606506348\n",
      "12.681976318359375\n",
      "13.608572006225586\n",
      "13.77905559539795\n",
      "13.749439239501953\n",
      "10.753203392028809\n",
      "12.711237907409668\n",
      "11.99191665649414\n",
      "11.687150001525879\n",
      "15.138042449951172\n",
      "12.507275581359863\n",
      "15.099626541137695\n",
      "12.280254364013672\n",
      "14.115315437316895\n",
      "12.988566398620605\n",
      "12.689032554626465\n",
      "11.973580360412598\n",
      "11.438942909240723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = score_estimator.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "losses = []\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    times = torch.rand(theta.shape[0], device=theta.device)*0.99 + 0.01\n",
    "    loss = score_estimator.loss(theta, x, times=times).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sbi.inference.trainers.fmpe.fmpe.FMPE at 0x7f36044d2cb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FMPE(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cond_emb torch.Size([10, 34])\n",
      "Input_layer Linear(in_features=34, out_features=100, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(thetas, xs, torch.ones((10,))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = torch.randn((1000, 10))\n",
    "xs = torch.ones((1000, 10))\n",
    "score_est = posterior_score_nn(\n",
    "    sde_type=\"vp\",\n",
    "    net=DummyNet(),\n",
    "    embedding_net=torch.nn.Identity(),\n",
    ")(thetas, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9996, -1.0026, -0.9992, -1.0046, -0.9968, -1.0009, -1.0007, -0.9986,\n",
       "         -0.9986, -0.9951]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_est(torch.ones((1, 10)), torch.ones((1, 10)), torch.ones((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cond_emb torch.Size([1000, 18])\n",
      "Input_layer Linear(in_features=18, out_features=100, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_est.net(thetas, xs, torch.ones((1000, 1))).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
