## Branching

From merge_flow_builders_to_current_main:
	-> simformer (actual branch for the PR)
	-> simformer-dev (quick tests)

## TODO

- Clean the codebase
    X Solve comments around codebase (# ? and # !)
    X Corrections by Manuel: Simformer in NeuralNets renamed in SimformerNet, etc. (search # ? and # !)
    - Adjust docstring copy/pasted
    - Optimize code
    - Remove stupid comments
    - Renaming conditioning_mask in conditioning_mask
    - Check that convention (1 =, 0=) for condition mask and (1=, 0=) for edge mask is respected
    X Move all Masked* versions of classes below their reference
    - standard_simformer or simformer? How to require ada?
- Generalize wrapper (not just score) and move to another file
- Condition and Edge mask multiple if handling in VE/ScoreEstimator loss
- Decide where you want to generate the automatic edge and condition masks
- Decide at what level you want to see masks to appear among parameters of methods, and where not, also decide accordingly the implementation of the Wrapper as it must be a *callable* by itself
- Fix ode_fn not passing condition and edge mask to the subsequent Score
- Fix score, forward, loss, ode_fn not receiving condition and edge mask in Wrapper withing MaskedConditionalVectorFieldEstimator.unmask() method
- Test extensively with complex shapes, re-check every change you make
- Finish up the joint classes if you have time
- Should simformer work in multi-round?

## Questions

In Diffuser.initialize() will this always be 1?

	num_batches = (
		1 if self.predictor.potential_fn.x_is_iid else self.batch_shape.numel()
	)

Also in plot.py I added .cpu() here:

def ensure_numpy(t: Union[np.ndarray, torch.Tensor]) -> np.ndarray:
    """
    Returns np.ndarray if torch.Tensor was provided.

    Used because samples_nd() can only handle np.ndarray.
    """
    if isinstance(t, torch.Tensor):
        return t.cpu().numpy()
    elif not isinstance(t, np.ndarray):
        return np.array(t)
    return t
